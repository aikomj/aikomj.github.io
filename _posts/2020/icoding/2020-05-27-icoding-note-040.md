---
layout: post
title: 飞天班第40节：ElasticSearch实战-3
category: icoding-edu
tags: [icoding-edu]
keywords: elasticSearch
excerpt: 实战项目jdsearch，搭建ES集群，ES必会面试题
lock: noneed
---

## 1、实战项目Search

流程：爬虫获取数据 => ES  => 前端搜索

定义索引的maping

```json
POST /jd_goods/_mapping
{
    "properties": {
        "title": {
            "type": "text",
            "analyzer": "ik_max_word"
        },
        "price": {
            "type": "text",
            "index": false
        },
        "keyword": {
        		"type": "text",
        		"analyzer": "ik_max_word"
        }
        "commit": {
            "type": "text",
            "index": false		// 评论数
        },
        "shop": {
            "type": "text",
            "analyzer": "ik_smart"
        },
        "img": {
            "type": "text",
            "index": false
        }
    }
}
```

项目结构

![](/assets/images/2020/icoding/elasticsearch/java-es-search-project1.gif)

测试页面

![](/assets/images/2020/icoding/elasticsearch/java-es-search-project.gif)



## 2、ES集群

> 核心概念

**Cluster**：集群，一个集群有很多个节点

**Shards：**分片，我们在创建索引的时候需要填写分片数和副本数，是因为ES会把一个完整的索引，分割成分片，这些分片存储在不同的节点上，就可以构成分布式搜索，达到高性能，高可用的目的。分片数在索引创建后不能修改，除非重建索引。

**Replicas：** 副本，分片的一个副本（拷贝），副本的作用；容错性，高可用。如果主分片坏了，ES会将一个副本分片中升级为主分片，再拷贝一个副本。副本分片可以分担读的负载，提高查询效率，这就是所谓的ES自带负载均衡

> 为什么要实现集群

- 业务量增加，索引变多，数据量变大，单个es服务器节点已无法支撑正常使用，查询变慢，存储变慢

- 采用ES集群来解决这些问题，我们可以将单个索引，分片到不同的节点（物理服务器）存储，横向扩展，实现高可用，高性能，高并发

ES集群中，索引是由 分片 和 副本构成，如下图，粗框为主分片，细框为副本分片

![](/assets/images/2020/icoding/elasticsearch/normal-cluster.gif)



### 集群的规划

> 1、我们需要多大规模（多少个节点）的集群

**考虑方向：**

1、当前的数据量有多大？数据量增长情况？（业务）

2、机器的配置？CPU、多大内存、硬盘容量（物理硬件）

**推算依据：**

1、ES 默认的 java heap占用内存消耗很大，最大可以设置 32G

2、32G 内存大概可以处理的数据量是 10T  ，如果你的服务器有128G内存，这个时候你可以在一个机器上运行多个ES节点构成集群（节点一般分布在不同服务器，避免服务器宕了，整个ES挂了）

==集群规划要满足当前数据的规模+数据的增长速度，后面按需扩展即可！==

**场景：**

1、构建业务搜索（垂直搜索），数据规模： 几千万 到 数十亿，  ES集群需要2~4 台机器即可。

2、大规模的数据 OLAP（联机处理分析）， ELK数据规模：千亿级别，  ES集群需要几十上百的节点。



> 2、集群中节点如何分配

**默认情况下，每个es节点都承担了以下功能：**

Master： 主节点

DataNode： 数据节点

coordinate node ： 协调节点 （只用来接收，转发请求到其他节点，汇聚所有的节点返回的数据等等....）

**规则：**

1、小规模集群， 不需要严格区分角色，也就是不分配。

2、中大规模集群（10个节点），如果压力很大，可以适当增加一些协调节点（分工，降低压力）

==配置文件配置下即可==

```shell
node.mater: true # 是主节点
node.data: true # 是数据节点
```



> 3、如何避免脑裂问题

**脑裂问题**

一个集群中只有一个主节点A，A节点十分忙。因为网络故障， 其他节点ping不通A节点。这样的话其他节点就认为 A 节点不可用，就会重新选举一个 B为主节点。这个时候A又正常了，就会出现两个主节点？ 数据可能一部分来自A，一部分来自B，出现数据不一致的问题，这就是脑裂。

**解决方案：**

`discovery.zen.minimum_master_nodes: `  配置最小候选master主节点票数

ES中master主节点是要经过多个有资格成为master（node.master=true）的节点选举后才能成为新的节点，不是你一个节点自己选自己就能决定的，当一个候选节点得到的票数 >=discovery.zen.minimum_master_nodes 的配置值，那它才能成为新的master节点。

这个配置值也叫 quorum数量，一般不用配置discovery.zen.minimum_master_nodes，当业务量很大的时候才需要配置。

<font color=red>官方推荐：discovery.zen.minimum_master_nodes = 有资格选举 master节点的数量/2+1</font>

**举例：**

1、如果有10个节点，都是data node，也是master的候选节点。则quorum=10/2+1=6

2、如果有3个master候选节点，100个数据节点。则quorum=3/2+1=2

3、如果有2个节点，都是data node，也是master的候选节点。则quorum=2/2+1=2（有问题）,无法防止脑裂，所以一个ES集群最少有3个节点。

```shell
discovery.zen.minimum_master_nodes除了在配置文件设置，也可以动态设置

PUT /_cluster/settings
{
   "persistent":{
      "discovery.zen.minimum_master_nodes":2
   }
}
```



**集群配置（常用）**

1、直接将 master 候选节点 和 data 数据节点分开，配置上奇数个的master候选节点 ， 最少3个。

2、配置单播，ping（默认是关闭的）

```yaml
discovery.zen.ping.multicast.enabled: false  # 关闭多播发现机制（默认就是关闭的）
discovery.zen.ping.unicast.hosts: ["localhost:9001","localhost:9101","localhost:9701"]
```

单播配置一些master候选节点的ip地址，其他的节点要加入进来。需要在这个单播配置的master候选节点中去等待统一。等待master主节点同意，就会自动加入集群。



> 4、如何设置索引的分片

**思考：**

分片对应的存储实体是什么？  底层本质就是一个 Lucene索引。

分片是不是越多越好？ 不是

分片多了有什么影响？存储空间浪费，占用资源，影响性能。

分片越多，每个分片的数据量就少，查询性能反而受影响 会下降

**规则：**

最大的ES内存推荐，32G。

1、200GB = 7/8个分片，索引大概200G数据量，7个分片就可以了

2、按照节点数量来分配；如 总共有3个节点，  3 * （1.5~3倍左右），那么分片数就是5个到9个， 尽量不要超过 9 个，以此类推计算。



> 5、如何设置副本数

默认一个副本就好，不超过2个副本。

作用：高可用，保证数据不丢失，高并发的时候可以参与查询。



### 搭建集群

核心就是修改elasticsearch.yml这个配置文件

下面模拟一个服务器上开启3个ES实例（节点）搭建ES集群，一个服务器一个ES实例（节点）最好，那就不用修改端口了。

```shell
# linux服务器上，复制elasticsearch，分别以端口命名
[esuser@helloworld ~]$ cp -r elasticsearch-7.6.1 elasticsearch-9201
[esuser@helloworld ~]$ cp -r elasticsearch-7.6.1 elasticsearch-9202
[esuser@helloworld ~]$ ls
elasticsearch-7.6.1  elasticsearch-9201  elasticsearch-9202  kibana-7.6.1
# 删除复制过来的数据和日志
[esuser@helloworld elasticsearch-9201]$ rm -rf data logs
# 重新创建数据目录和日志目录
[esuser@helloworld elasticsearch-9201]$ mkdir data logs
# 修改配置文件
```

elasticsearch.yml配置集群参数，端口ip自行修改

```shell
# 1、集群名称，
cluster.name: coding-es

# 2、节点名和节点作用
node.name: node-3
# 是否有资格被选举成为master节点（是不是master候选节点）
node.master: true
# 是否存储索引的数据
node.data: true
# 存储路径
path.data: /usr/local/elasticsearch/esdata
path.logs: /usr/local/elasticsearch/eslogs

# 绑定可以访问的ip, 如果是云服务器，配置0.0.0.0,不限制访问的ip地址
network.host: 127.0.0.1
# http访问端口（ 如 elasticsearch head, kibana就是通过该端口访问ES）,默认是9200
http.port: 9202
# tcp访问端口（集群内节点通信端口，如主分片的数据复制到副本分片），默认是9300
transport.tcp.port: 9302

# discovery.zen.ping.multicast.enabled: false  # 关闭多播发现机制（默认就是关闭的）
# 3、候选master节点列表
discovery.zen.ping.unicast.hosts: ["127.0.0.1:9300","127.0.0.1:9301","127.0.0.1:9302"]
# es 7.x版本的参数配置可以使用这个
discovery.seed_hosts: ["127.0.0.1:9300", "127.0.0.1:9301","127.0.0.1:9302"]


# 初始主节点，为节点名
cluster.initial_master_nodes: ["node-1"]

# 配置跨域
http.cors.enabled: true
http.cors.allow-origin: "*"
```

启动集群，

```shell
[esuser@helloworld ~]$ ./elasticsearch-7.6.1/bin/elasticsearch -d
[esuser@helloworld ~]$ ./elasticsearch-9201/bin/elasticsearch -d  
[esuser@helloworld ~]$ ./elasticsearch-9202/bin/elasticsearch -d 
# 检查是否都启动成功
[esuser@helloworld ~]$ ps -ef|grep elastic
# 云服务器上防火墙和安全组要开放端口 9200,9201,9202，才能外网访问

```



elasticsearch head 访问其中一个节点，星标记的节点为集群的master节点

![](/assets/images/2020/icoding/elasticsearch/cluster1.png)

添加一个节点，分片会自动调整到对应的节点上

![](/assets/images/2020/icoding/elasticsearch/cluster2.png)



删除两个节点试试，分片也会自动调整，

![](/assets/images/2020/icoding/elasticsearch/cluster3.png)

可以看到node-1 的主分片P3 会复制一份副本R3放到node-3节点上，副本分片R4会升级为P4，然后复制一份副本R4放到node-3节点上。node-3的副本分片R0、R1会升级为P0、P1，然后复制一份副本放到node-1节点上，主分片P2复制一份副本R2放到node-1节点上，这样就完成了整个索引的分片调整。



ES集群对与客户端来说就是一个整体的服务

![](/assets/images/2020/icoding/elasticsearch/client-to-cluster.png)

### 核心原理

1、每个索引会被分片存储，分配到不同的集群节点中，默认是5个分片。

2、高可用，我们就需要给每个分片设置副本，副本分片可以分担查询请求的负载。

3、存放文档路由规则，hash规则

![](/assets/images/2020/icoding/elasticsearch/cluster-node-hash-shards.png)

4、ES 的核心存储： 索引（分片存储）->文档

5、ES一旦使用集群，分片就归集群管理（完全是自动化的）

6、高可用例子：

![](/assets/images/2020/icoding/elasticsearch/cluster-one-node-down.png)

## 3、面试题学习

[https://www.jianshu.com/p/ad71c7518d58](https://www.jianshu.com/p/ad71c7518d58)

[https://yq.aliyun.com/articles/740770](https://yq.aliyun.com/articles/740770)

[https://zhuanlan.zhihu.com/p/102500311](https://zhuanlan.zhihu.com/p/102500311)

[https://blog.csdn.net/zl1zl2zl3/article/details/89035904](https://blog.csdn.net/zl1zl2zl3/article/details/89035904)

